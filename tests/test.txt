Loading the model...
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ../../models/zephyr-7b-alpha.Q4_K_M.gguf (version GGUF V2)
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-alpha
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V2
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) 
llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-alpha
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.15 MiB
llm_load_tensors:        CPU buffer size =  4165.37 MiB
...............................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB
llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB
llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB
llama_new_context_with_model: graph nodes  = 1030
llama_new_context_with_model: graph splits = 1
AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'huggingfaceh4_zephyr-7b-alpha', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}
Using fallback chat format: llama-2
Model loaded.
Running model...

llama_print_timings:        load time =   13855.02 ms
llama_print_timings:      sample time =      37.56 ms /    95 runs   (    0.40 ms per token,  2528.95 tokens per second)
llama_print_timings: prompt eval time =   13851.89 ms /    57 tokens (  243.02 ms per token,     4.11 tokens per second)
llama_print_timings:        eval time =   23699.66 ms /    94 runs   (  252.12 ms per token,     3.97 tokens per second)
llama_print_timings:       total time =   37694.32 ms /   151 tokens
{
  "id": "cmpl-940df70d-5bc1-451b-9e88-990f62fd7a03",
  "object": "text_completion",
  "created": 1719313441,
  "model": "../../models/zephyr-7b-alpha.Q4_K_M.gguf",
  "choices": [
    {
      "text": "Question: Como experto en lingüística, proporciona cinco frases utilizando el sustantivo 'tierra' en género masculino con el sentido de 'la parte sólida de la superficie de la Tierra'. Answer: 1. La tierra es un elemento fundamental para la existencia humana. 2. La tierra es el hogar de los seres vivos. 3. La tierra es una fuente inagotable de recursos naturales. 4. La tierra es un recurso que debemos cuidar y proteger. 5. La tierra es la base de nuestra civilización y cultura.",
      "index": 0,
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 57,
    "completion_tokens": 94,
    "total_tokens": 151
  }
}
Running model...
Llama.generate: prefix-match hit

llama_print_timings:        load time =   13855.02 ms
llama_print_timings:      sample time =      64.56 ms /   149 runs   (    0.43 ms per token,  2307.93 tokens per second)
llama_print_timings: prompt eval time =    5554.88 ms /    25 tokens (  222.20 ms per token,     4.50 tokens per second)
llama_print_timings:        eval time =   37130.01 ms /   148 runs   (  250.88 ms per token,     3.99 tokens per second)
llama_print_timings:       total time =   42925.73 ms /   173 tokens
{
  "id": "cmpl-949c0f94-1664-4199-9793-59e655f04455",
  "object": "text_completion",
  "created": 1719313479,
  "model": "../../models/zephyr-7b-alpha.Q4_K_M.gguf",
  "choices": [
    {
      "text": "Question: Como experto en lingüística, proporciona cinco frases utilizando el sustantivo 'tierra' en género femenino con el sentido de 'la parte sólida de la superficie de la Tierra'. Answer: 1. La tierra es un recurso valioso que debemos cuidar y proteger. 2. La tierra es nuestro hogar, nuestra morada en este planeta. 3. La tierra nos proporciona comida, agua y aire para respirar. 4. La tierra es un lugar de belleza y diversidad, con montañas, ríos, bosques y playas. 5. La tierra es nuestro legado para las generaciones futuras, y debemos hacer lo mejor posible para que puedan disfrutar de ella como nosotros la hemos disfrutado.",
      "index": 0,
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 58,
    "completion_tokens": 148,
    "total_tokens": 206
  }
}
Running model...
Llama.generate: prefix-match hit

llama_print_timings:        load time =   13855.02 ms
llama_print_timings:      sample time =      47.37 ms /   116 runs   (    0.41 ms per token,  2448.81 tokens per second)
llama_print_timings: prompt eval time =    4269.74 ms /    19 tokens (  224.72 ms per token,     4.45 tokens per second)
llama_print_timings:        eval time =   28807.55 ms /   115 runs   (  250.50 ms per token,     3.99 tokens per second)
llama_print_timings:       total time =   33242.30 ms /   134 tokens
{
  "id": "cmpl-eefdb786-1d1f-402b-aec4-c1f55dc8954e",
  "object": "text_completion",
  "created": 1719313522,
  "model": "../../models/zephyr-7b-alpha.Q4_K_M.gguf",
  "choices": [
    {
      "text": "Question: Como experto en lingüística, proporciona cinco frases utilizando el sustantivo 'piloto' en género masculino con el sentido de 'NULL'. Answer: 1. El piloto nulo no está presente en la cabina. 2. La falta de un piloto nulo es una característica importante del avión. 3. El piloto nulo es necesario para simular situaciones extremas en los entrenamientos. 4. El piloto nulo es útil para probar la seguridad de los sistemas de control. 5. La falta de un piloto nulo puede ser una falla en el sistema de aviónica.",
      "index": 0,
      "logprobs": null,
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 46,
    "completion_tokens": 115,
    "total_tokens": 161
  }
}
Running model...
Llama.generate: prefix-match hit

llama_print_timings:        load time =   13855.02 ms
llama_print_timings:      sample time =      84.68 ms /   200 runs   (    0.42 ms per token,  2361.94 tokens per second)
llama_print_timings: prompt eval time =    3180.52 ms /    13 tokens (  244.66 ms per token,     4.09 tokens per second)
llama_print_timings:        eval time =   50591.27 ms /   199 runs   (  254.23 ms per token,     3.93 tokens per second)
llama_print_timings:       total time =   54087.65 ms /   212 tokens
{
  "id": "cmpl-3ad57e8d-6c76-464b-9d80-69306900864f",
  "object": "text_completion",
  "created": 1719313555,
  "model": "../../models/zephyr-7b-alpha.Q4_K_M.gguf",
  "choices": [
    {
      "text": "Question: Como experto en lingüística, proporciona cinco frases utilizando el sustantivo 'piloto' en género femenino con el sentido de 'NULL'. Answer: 1. La pilota es una bola pequeña y redonda utilizada en deportes como tenis o fútbol. Sin embargo, en este contexto, la palabra \"pilota\" se utiliza como un sustantivo femenino que significa \"nulo\", \"inexistente\", o \"inexistente\".\n\n2. La pilota de la nave espacial no respondió a las comunicaciones. Es decir, no hubo respuesta desde el piloto de la nave.\n\n3. El sistema informático no detectó la presencia del piloto en la red. En otras palabras, no hay conexión entre el piloto y el sistema.\n\n4. La pilota de la computadora no se ha encontrado en el sistema. Es decir, no hay ninguna actividad o proceso que se pueda atribuir",
      "index": 0,
      "logprobs": null,
      "finish_reason": "length"
    }
  ],
  "usage": {
    "prompt_tokens": 47,
    "completion_tokens": 200,
    "total_tokens": 247
  }
}
